# Microsoft-Malware-Detection

The whole code was run on GCP system with below configuration
 Processor: 16 core AMD
 RAM: 64 GB
 OS: Ubuntu
 Hard disk: 1000 GB

There were total 10863 files of two type
  1. byte
  2. asm

For feature extration we build a custom word2vec due to the large size of data. In the byte file we extrated unigram, bi-gram and tri-gram. For bi-gram and tri-gram we use something other and the tradition method inspired by https://programminghistorian.org/en/lessons/keywords-in-context-using-n-grams which helped us reduce the dimension.
We also followed the tradition method which would have given 66000 bi-gram features and then used Randomforest to get import features.

The models used are Logistic Regression, Random forest and Xgboost.

As the number of files and size of those files were large uwe used python multithreading for faster feature extraction.

Conclusion:
1. Using multi-threading and dividing the les into different folders decrease the feature
extraction time exponential.
2. When we t the features from both the le separately asm featues has lower log loss than
byte features.
3. Xgboost was the best performing model and logistic regression was the worst. Even
thought the asm le features were performing good but they were overtting.
4. Logistic Regression was the worst performing model in any situation and was not able to
predict class 5.
5. Random Forest was giving better performance tha Logistic Regression but in case of
tting both the les features separatly the model was overtting and on combining both
the features the overtting decresed.
6. Xgboost was the best performing model in any scenario when using separately on both the
les there was some overtting but the log loss was always less than 0.08 but on
combinig the features we were not only able to remove the overttting, we achieved a log
loss of 0.01 in train, cv and testing which was better than any of the models used.

7. We can further reduce the log loss if we implement the same bi-gram and tri-gram technique on asm files.
